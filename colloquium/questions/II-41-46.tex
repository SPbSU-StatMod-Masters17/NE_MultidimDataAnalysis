\subsection{Билет 41 Объясненные	каноническими	переменными	доли	дисперсии.	Избыточность.}
Пусть $\overline{\xi} \in \R^p, \ \overline{\eta} \in \R^q$ --- исходные признаки.
$A_i^T \overline{\xi}, \ B_i^T \overline{\eta}$ --- $i$-ые канонические направления.
Утверждается, что, если $\mathbb{E}\xi_i = 0, \mathbb{D}\xi_i = 1, \ \mathbb{E}\eta_i = 0, \mathbb{D}\eta_i = 1 \ \forall i$, то
\begin{equation*}
\sum\limits_{k = 1}^{s} \rho^2(\eta_i, B_k^T \overline{\eta}) \le 1 = \mathbb{D}\eta_i, \ \sum\limits_{k = 1}^{s} \rho^2(\xi_i, A_k^T \overline{\xi}) \le 1 = \mathbb{D} \xi_i , \ s = \min(p, q).
\end{equation*}
При этом, если $s = q$, то в первом случае достигается равенство, а если $s = p$, то во втором.
Поэтому $\sum\limits_{k = 1}^{s} \rho^2(\eta_i, B_k^T \overline{\eta})$ и $\sum\limits_{k = 1}^{s} \rho^2(\xi_i, A_k^T \overline{\xi})$ называется объясненной долей дисперсии.

\textbf{Избыточность:}
Избыточностью $i$-ого левого признака $\xi_i$ называется
\begin{equation*}
\sum\limits_{k = 1}^{s} \rho^2(\xi_i, B_k^T \overline{\eta}).
\end{equation*}
Аналогично для правых. На консультации мы не разобрались, как ее интерпретировать, поэтому достаточно только определения.

\subsection{Билет 42.Что общего	между	дискриминантным	анализом в многомерной	множественной	регрессией?}
Окей, давайте по порядку.
Задача MANOVA (буду сразу писать в нормальной модели с равными ковариационными матрицами, а не начинать с общей задачи на равенство распределений):

Пусть $\eta_i \sim \mathcal{N}(\mu_i, \Sigma)$, $i = 1, \ldots k$.

Необходимо проверить гипотезу
\begin{equation*}
H_0: \mu_1 = \ldots = \mu_k.
\end{equation*}
Введем одномерный качественный признак $\xi$, принимающий значения $A_1, \ldots A_k$. Тогда можно рассмотреть пару $(\eta, \xi)$, такую что $\mathcal{P}_{\eta \mid \xi = A_i} = \mathcal{P}_{\eta_i}$ и гипотеза принимает вид
\begin{equation*}
H_0: \mathbb{E}(\eta \mid \xi = A_1) = \ldots = \mathbb{E}(\eta \mid \xi = A_k).
\end{equation*}
Что эквивалентно независимости $\eta$ от $\xi$
Знаем, что
\begin{equation*}
\mathbb{E}(\eta - \mathbb{E}\eta)(\eta - \mathbb{E}\eta)^T = \mathbb{E}(\mathbb{E}(\eta \mid \xi) - \mathbb{E}\eta)(\mathbb{E}(\eta \mid \xi) - \mathbb{E}\eta)^T + \mathbb{E}(\eta - \mathbb{E}(\eta \mid \xi))(\eta - \mathbb{E}(\eta \mid \xi))^T
\end{equation*}
То есть, на самом деле мы можем проверять гипотезу о том, что
\begin{equation*}
H_0: \mathbb{E}(\mathbb{E}(\eta \mid \xi) - \mathbb{E}\eta)(\mathbb{E}(\eta \mid \xi) - \mathbb{E}\eta)^T = 0
\end{equation*}
Теперь перейдем на выборочный язык и напишем основное дисперсионное тождество ($n_i$ -- длина выборки для $i$-ой группы.
\begin{equation*}
\sum\limits_{i = 1}^{k}\sum\limits_{j = 1}^{n_i}(y_{ij} - \overline{y})^2 = \sum\limits_{i = 1}^{k}n_i(y_{ij} - \overline{y}_{i.})^2  + \sum\limits_{i = 1}^{k}\sum\limits_{j = 1}^{n_i}(\overline{y}_{i.} - \overline{y})^2 = \mathbb{H} + \mathbb{E}
\end{equation*}
Статистика критерия, проверяющая нашу гипотезу имеет вид
\begin{equation}
\Lambda = \frac{|\mathbb{E}|}{|\mathbb{E} + \mathbb{H}|} = \prod\limits_{i = 1}^s (\frac{1}{1 + \lambda_i}),
\label{maov}
\end{equation}
где $\lambda_i$ -- собственные числа матрицы $\mathbb{E}^{-1} \mathbb{H}$, $s = \min(p, k-1).$

Теперь введем переменные $\zeta_1, \ldots, \zeta_{k-1}$ как dummy variables для переменной $\xi$ и рассмотрим многомерную множественную линейную регрессию вектора $\overline{\zeta}$ на $\eta$.
А именно рассмотрим
\begin{equation*}
\mathbb{Y} = \mathbb{X}\mathbb{B} + \Xi
\end{equation*}
Предположим, что $\mathbb{Y}$ и $\mathbb{X}$ --- центрированы.
Решение данной задачи $\hat{\mathbb{B}} = (\mathbb{X}^T\mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}$.
Значимость регрессии проверяется гипотезой
\begin{equation*}
H_0: \mathbb{B} = 0.
\end{equation*}
Положим $\mathbb{E} = (\hat{\mathbb{Y}} - \mathbb{Y})(\hat{\mathbb{Y}} - \mathbb{Y})^T$,
$\mathbb{H} = \hat{\mathbb{Y}}\hat{\mathbb{Y}} ^T$ и статистика критерия имеет вид
\begin{equation}
\Lambda = \frac{|\mathbb{E}|}{|\mathbb{E} + \mathbb{H}|} = \prod\limits_{i = 1}^s \left(\frac{1}{1 + \lambda_i}\right) = \prod(1 - r^2_i),
\label{regr}
\end{equation}
где $r_i$ --- это $i$-ая каноническая корреляция.
Так вот, оказывается, что $\Lambda$ из \eqref{maov} совпадает с  $\Lambda$ из $\eqref{regr}$.
Таким образом получается, что $i$-ая каноническая корреляция между наборами $\zeta_1, \ldots, \zeta_{k-1}$ и $\eta_1, \ldots, \eta_p$ выражается через собственные числа $\lambda_i$, полученные в результате дискриминантного анализа как
\begin{equation*}
r_i^2 = \frac{\lambda_i}{1 + \lambda_i}
\end{equation*}
\subsection{Билет 43.Две группы, использование	множественной линейной регрессии для классификации.}
Все в тех же обозначениях положим $k = 2$. Таким образом у нас получается только одна dummy-переменная и мы можем рассмотреть регрессию $\eta_1, \ldots, \eta_p$ на $\zeta_1$.
(Мы все перевернули, но это не смертельно ибо в канонических корреляциях все симметрично).
В таком случае первая каноническая корреляция $r^2_1 = R(\zeta, \eta_1, \ldots, \eta_p)$ --- это выборочный коэффициент корреляции.

\begin{thm}
Канонические коэффициенты $A_1$ пропорциональны $B^{(c)}$, которые получены при решении регрессионной задачи и пропорциональны $\Sigma^{-1}(\mu_1 - \mu_2)$.
\end{thm}
Вообще это частично показывалось, но я так поняла, что это не нужно..
\subsection{Билет 44.Кластерный	анализ,	пример	model-based	подхода}
Предположим, что многомерная выборка --- неоднородная. Но в отличие от дискриминантного анализа у нас нет признака, объясняющего эту неоднородность и задачей является ее выявить.
Тип классификации, когда есть модель называется model-based clustering.
Например, пусть наша выборка из смеси $k$ нормальных распределений. Таким образом ее плотность имеет вид
\begin{equation}
p(x) = \pi_1 p(x, \mu_1, \Sigma_1) + \ldots + \pi_k p(x, \mu_k, \Sigma_k),
\end{equation}
где
\begin{equation}
p(x, \mu_i, \Sigma_i) = \frac{1}{(2\pi)^{p/2}\sqrt{|\Sigma_i|}}
\exp \left(-\frac{1}{2}(x - \mu_i)\Sigma_i^{-1}(x - \mu_i)^T \right)
\end{equation}
Эта задача решается методом максимального правдоподобия с помощью ЕМ-алгоритма.

\subsection{Билет 45.Кластерный	анализ:k-means,	k-means++}
Хотим искать кластеры $C_1, \ldots, C_k$ минимизируя следующий функционал
\begin{equation}
\sum\limits_{i = 1}^k \sum\limits_{j \in C_i} ||x_j - \mu_i||
\label{func_lin}
\end{equation}
по разбиению всего пространства на $C_j$ и по всем $\mu_i$.
Можно делать это по следующему алгоритму:
\begin{enumerate}
\item Выбираем случайно $\mu_1, \ldots, \mu_k$.
\item $C_j$ -- кластер, содержащий точки, которые лежат к $\mu_j$ ближе, чем к остальным $\mu_i$.
\item Для каждого $C_j$ пересчитываем центр как выборочное среднее элементов из этого кластера.
\end{enumerate}
Проблема метода в том, что у такого функционала имеет много локальных минимумов и алгоритм может сойтись в значение, далекое от истинного.
Метод $k-means ++$ повторяет алгоритм, приведенный выше, но начальные значения выбираются не случайно, а следующим образом
\begin{enumerate}
\item Выбираем случайным образом первый центр $\mu_1$.
\item Считаем расстояние от всех точек до ближайшего центра $\{\rho_i\}$. После чего выбираем $x_i$ как новый центр с вероятностью, пропорциональной $\rho_i$.
\item Пока количество центров меньше, чем $k$, повторяем процедуру.
\end{enumerate}
Результат полученной процедуры запишем как $J(\{C_j\}, \{\mu_j\})$.
Известно, что
\begin{equation*}
\frac{\mathbb{E}(J(\{C_j\}, \{\mu_j\}))}{J_{min}} = O(\ln k ).
\end{equation*}
\subsection{Билет 46. Запись задачи, решаемой k-means, как задачи low-rank approximation с	ограничениями. Использование PCA}
Функционал вида \eqref{func_lin} можно переписать в виде
\begin{equation}
    ||\mathbb{X} - \mathbb{G}\mathbb{M}||^2_F,
\end{equation}
где $M = (\mathbb{m}_1, \ldots, \mathbb{m}_k)$, $\mathbb{G}$ --- матрица, у которой в каждой строке
стоит ровно одна единица и остальныеa нули.
При этом, ранг матрицы $\mathbb{G}\mathbb{M}$ не превосходит $k$, что приводит нас к задачи
аппроксимации заданной матрицы матрицей меньшего ранга.  Заметим, что задача похоже на задачу
сингулярного разложения с ограничениями\footnote{Матрица $\mathbb{G}$ должна «вытаскивать» вектор среднего для соответсвующей строчки $\mathbb{X}$}
Есть результаты, что если к данным применить анализ главных компонент, то пространство, натянутое на первые $k-1$ главных векторов будет близко к пространству, проходящему, через центры кластеров.
