{\color{blue} Набирал Вася 15-21. Кажется обозначения должны совпадать с 22-35, хотя мог что-нибудь пропустить.}
\subsection{Билет 15. Оптимальность в анализе главных компонент в статистической терминологии (через дисперсии)}

\begin{dfn} Будем говорить, что $w \in \R^p$ задает первое главное направление, если 
\begin{enumerate}
\item $||w||_2 = 1$
\item $w = \argmax\limits_{w \in \R^p}\sum \limits_{i=1}^n <y_i, w>^2 $
\end{enumerate}
\end{dfn}



\begin{dfn} Пусть $w_1, …, w_{s-1}$ — главные направления. Будем говорить, что $w \in \R^k$ задает $s$-ое главное направление, если 
\begin{enumerate}
\item $||w||_2 = 1$
\item $w_s = \argmax\limits_{w}\sum \limits_{i=1}^n <y_i, w>^2 s.t. w \perp w_i \forall i=1…(s-1) $
\end{enumerate}
\end{dfn}


\begin{thm}
В обозначениях, введенных выше:

\begin{enumerate}
\item $\max\limits_{w}\sum \limits_{i=1}^n <y_i, w>^2  = \lambda_1$
\item $ \argmax\limits_{w}\sum \limits_{i=1}^n <y_i, w>^2 = u_1$
\item Для $s$-ого главного направления аналогично 
$$ \argmax\limits_{\substack{w_s \in \R^p \\ w_s \perp w_i\\ i=1…(s-1)}}\sum \limits_{i=1}^n <y_i, w_s>^2 = u_s$$
$$ \max\limits_{\substack{w_s \in \R^p\\ w \perp w_i\\ i=1…(s-1)}}\sum \limits_{i=1}^n <y_i, w_s>^2 = \lambda_s$$
\end{enumerate}
\end{thm}

Данное утверждение — переформулировка экстремальной задачи, определяющей SVD.

Заметим, что за счет центрирования $\sum \limits_{j=1}^n <y_j, u_i>^2$ с точностью до константы совпадает с выборочной дисперсией признака $z_j = <y_j, u_i>$ ($z_i$ — линейная комбинация изначальных признаков). Таким образом, 
первое главное направление в статистической терминологии — такая прямая, при проекции на которую получившийся признак будет иметь максимальную выборочную дисперсию. 
$s$-ые направления аналогично, при условии, что ищем прямую, перпендикулярную всем предыдущим.

\subsection{Билет 16. Оптимизация в АГК в терминах ковариационных матриц.}

\begin{thm}
Следующие две задачи эквивалентны:
\begin{enumerate}
\item
$$  \argmin\limits_{\widetilde{Y}, rk\widetilde{Y} \leq d} ||Y - \widetilde{Y}||_{F} $$
\item $$\argmin\limits_{\widetilde{Y}, rk\widetilde{Y} \leq d} ||YY^{\intercal} - \widetilde{Y}\widetilde{Y}^{\intercal}||_{F} = \argmin\limits_{\widetilde{S}, \widetilde{S} sym.,p.d., rk(\widetilde{S}) \leq d} ||S - \widetilde{S}||_{F}$$
\end{enumerate}
\end{thm}

Почему это так? Мы знаем решение как первой, так и второй задачи. Для первой — через SVD, для второй — через жорданову форму (или как частный случай SVD) – записываем матрицу в виде нужного разложения, обнуляем наименьшие собственные числа и получаем матрицу необходимого ранга, которая решает либо первую, либо вторую задачу:

Для первой задачи:
$$ Y = U\Lambda^{\frac{1}{2}} V^{\intercal}$$

Для второй:
$$ YY^{\intercal} = U\Lambda U^{\intercal}$$



\subsection{Билет 17. В двух статистических пакетах получились разные главные компоненты. Отчего так могло
получиться?}
Смотрим на разложение матрицы. 
$$ \mathbb{Y} = \sum \limits_{i=1}^{p} \sqrt{\lambda_i}u_iv_i^{\intercal}$$
Как мы знаем, $\lambda_1 \geq \lambda_2\geq…\geq \lambda_p\geq0$

Данное разложение определено не совсем единственно: Для собственных чисел кратности 1 $u_i$ и $v_i$ можно одновременно умножить на $-1$ и ничего не поменяется (а для комплексных чисел поворотов еще больше…), а 
для собственных чисел, у которых кратность больше 1, можно выбрать любой базис линейного подпространства, соответствующего данному собственному числу и будут получаться различные разложения. 

В стат. пакетах выч. методы, с помощью которых ищутся главные компоненты различаются, поэтому и найденные ими компоненты могут отличаться (а некоторые выч. методы могут и от запуска к запуску выдавать разные результаты, так что на самом деле и в одном стат. пакете теоретически можно получать разные результаты.)

\subsection{Билет 18. Смысл первой ГК, если все ковариации (корреляции) исходных признаков положительны.}
\begin{thm}[Перрон-Фробениус]
Пусть у матрицы $M$ все элементы строго-положительны. Тогда у матрицы $M$ существует положительное собственной число единичной кратности и можно выбрать такой собственный вектор для данного с.ч., что все его компоненты будут строго положительны.
\end{thm}
Данная теорема, примененная для ковариационной матрицы, дает нам, что первое собственное направление будет взвешенной суммой исходных признаков. Таким образом, ее смысл — «среднее всех признаков». 

\subsection{Билет 19. Разница между АГК по корреляционной и по ковариационной матрице на примере двух признаков.
Когда что использовать.}  

Пусть сначала у нас признаки стандартизированы. Тогда корреляционная матрица выглядит следующим образом:
$$\mathbb{S} = \mathbb{X}^{\intercal} \mathbb{X} = \begin{pmatrix}
1 & \rho \\
\rho & 1
\end{pmatrix}
$$

Ее собственные числа и вектора легко посчитать:
$$\lambda_1  = 1+\rho, u_1 =(2^{-1/2}, 2^{-1/2})^{\intercal}$$
$$\lambda_2 = 1-\rho, u_2 = (2^{-1/2},-2^{-1/2})^{\intercal}$$

Главные компоненты соответственно\footnote{запись в векторном стиле: $x_i$ — вектор-столбец для 1-ого признака}:
 $$ z_1 = \frac{x_1+x_2}{\sqrt{2}}$$
 $$z_2 = \frac{x_1-x_2}{\sqrt{2}}$$

 А теперь пусть у нас дисперсия первой компоненты будет «большой»:
 $$\mathbb{S} = \mathbb{X}^{\intercal} \mathbb{X} = \begin{pmatrix}
a^2 & a\rho \\
a\rho & 1
\end{pmatrix}
$$

Матрица $2\times 2$, для нее можно выписать формулы для собственных чисел и векторов. Теперь если устремить $a \rightarrow \infty$, то  будет выполнено следующее:
$$\frac{u_{11}}{u_{12}} \rightarrow \frac{a}{\rho}$$
$$\frac{\lambda_1}{\lambda_1+\lambda_2} \rightarrow 1$$
Вспомним, что $u_{1}$ — координаты первой главной компоненты в старом базисе и получим, что первый признак будет «перетягивать» на себя главное направление. И чем меньше корреляция, тем сильнее будет вклад первого признака в первую компоненту. 

Как выбрать: 
\begin{enumerate}
\item Признаки измеряны в «разных» единицах или вообще ничего про них не знаем — стандартизуем
\item Считаем, что признаки в «примерно одинаковых» единицах измерения — можем не стандартизовать.
\end{enumerate}

 \subsection{Билет 20. Способы выбора числа главных компонент}
Все методы во-многом эвристики. АГК - непараметрический, у нас нету модели, из-за чего никаких гипотез не проверить.

\begin{enumerate}
\item Scree plot — рисуем график: собственное число по оси $y$, номер с.ч. по оси $x$. Смотрим, где график начинает напоминать прямую, параллельную оси $x$ и берем компоненты до этого места в качестве результаты.
\item Воспользуемся тем, что $\sum \lambda_i$ равняется общей дисперсии (total variance). Поэтому 
$\frac{\sum_{i=1}^{d} \lambda_i}{\sum_{i=1}^{p}\lambda_i}$ можно трактовать, как процент «объясненной» дисперсии. Фиксируем, хотя бы сколько процентов хотим объяснить и на основе этого выбираем $d$.
\item Правило Кайзера. Берем $\lambda_i > \frac{tr(S)}{k}$, где $S$ — ковариационная матрица. Т.к. след матрицы равняется сумме ее собственных чисел, мы на самом деле берем те собственные числа, которые «выше среднего». 
В случае, когда анализ ведется по корреляционной матрицы, диагональ $S$ равна 1, поэтому критерий упрощается до $\lambda_i > 1$. 
\item Правило сломанной трости. Пусть $l_i = \frac{\lambda_i}{\sum\lambda_i}$. $\sum l_i = 1$. Кинем $n-1$ случайную точку в отрезке $[0,1]$ и получим $n$ отрезков. Отсортируем отрезки по длине и получим $L_1 \geq L2,…,\geq L_n$. Можно посчитать $EL_i = \frac{1}{n}\sum\limits_{j=i}^{n} \frac{1}{j}$. 
Найдем максимальное $d$  такое, что выполнено $l_i > EL_i \forall i = 1…d$ и возьмем в качестве главных компонент $u_1…u_d$. 
\item Выбираем столько, сколько можем объяснить.  
\end{enumerate} 

\subsection{Билет 21. Почему доля собственного числа по отношению к сумме собственных чисел называется
объясненной долей общей дисперсии?}
Запишем разложение по главным компонентам:
$$ \mathbb{X}^{\intercal}=\mathbb{Y} = \sum \limits_{i=1}^{p} \sqrt{\lambda_i}u_iv_i^{\intercal}$$
Как мы знаем, в этом разложении $u_i$ — собственные вектора, а $\lambda_i$ — собственные числа матрицы ковариаций (или корреляций), равной $\frac{\mathbb{X}^{\intercal}X}{n}$.

Запишем суммарную дисперсию (данные центрированы):
$$ \sum_{i=1}^{p} var_i= \sum_{i=1}^{p} \frac{1}{n}\sum_{j=1}^{n}\mathbb{X}_{ji}^2 = \frac{tr{X^{\intercal}X}}{n} = total variance$$
В АГК мы ищем собственные числа и собственные вектора как раз матрицы $\frac{1}{n}X^{\intercal}X$, след матрицы при смене базиса не меняется, поэтому $\sum \lambda_i = total variance$. 
Поэтому $\frac{\lambda_i}{\sum\limits_{i=1}^{p}\lambda_i}$ можно называть объясненной долей общей дисперсии. 
