\subsection{ Билет 15. MANOVA. Модель, запись через условные мат. ожидания η и мат.ожидания
η(k). Разложение ковариационной матрицы. }
%не надо в заголовках математику, она с hypperlink плохо
%работает
Этот билет во многом аналогичен части билета 10 про ANOVA.
Основная разница заключается лишь в том, что здесь $\eta$ является многомерным вектором.

Общая постановка задачи.
Пусть есть $k$ групп и $p$ признаков, мы пытаемся проверить, что группы друг от друга не отличаются.
Формально есть $k$ (многомерных) случайных векторов $\overline \eta_1, \ldots, \overline \eta_k \in \R^p$ и гипотеза формулируется так:

\begin{equation}
    \label{PreMANOVA}
    \mathrm H_0: \mathcal P(\overline \eta_1) = \mathcal P(\overline \eta_2) = \ldots \mathcal = P(\overline \eta_k).
\end{equation}
Эту задачу можно переформулировать следующим образом.
Пусть есть дискретный (м.б. качественный) признак $\xi$, принимающий ровно $k$ значений: $A_1, A_2, \ldots, A_k$.
Рассмотрим случайный вектор $\Tr{(\overline \eta, \xi)} \in \R^k \times \{A_1, A_2, \ldots, A_k\}$,
такой что $\mathcal P(\overline \eta \, | \, \xi = A_i) = \mathcal P(\eta_i)$ для всех $i \in 1:k$.
Тогда гипотеза \eqref{PreANOVA} переписывается в виде:
\begin{equation}
    \label{PreMANOVA2}
    \mathrm H_0^*: \mathcal P(\overline \eta \, | \, \xi = A_i) = \mathcal P(\overline \eta \, | \, \xi = A_j) \text{ для всех $i, j$}.
\end{equation}

В MANOVA рассматривается частный случай (модель), когда $\mathcal P(\overline \eta_i) = \mathcal N(\overline \mu_i, \Sigma)$.
В рамках этой модели гипотезам \eqref{PreMANOVA} и \eqref{PreMANOVA2} равносильны следующие две\footnote{равносильные} гипотезы:
\begin{gather}
    \label{MANOVA}
    \mathrm H_0: \overline \mu_1 = \overline \mu_2 = \ldots = \overline \mu_k.\\
    \mathrm H_0^*: \mathbb E (\overline \eta \, | \, \xi = A_1) = \ldots = \mathbb E (\overline \eta \, | \, \xi = A_k).
    \nonumber
\end{gather}

Прежде чем сформулировать гипотезу в рамках Основного Дисперсионного Тождества \eqref{VI} запишем его на генеральном языке
в текущей модели.
\begin{gather}
    \label{MANOVA_VI_gen}
    \mathrm {Cov} \overline \eta =
    \mathbb E \big[(\overline \eta - \mathbb E \overline \eta) \Tr{(\overline \eta - \mathbb E \overline \eta)}\big] = \\ =
    \mathbb E \big[(\overline \eta - \mathbb E (\overline \eta \, | \, \xi))
              \Tr{(\overline \eta - \mathbb E (\overline \eta \, | \, \xi))}\big] +
    \mathbb E \big[(\mathbb E (\overline \eta \, | \, \xi) - \mathbb E \overline \eta)
              \Tr{(\mathbb E (\overline \eta \, | \, \xi) - \mathbb E \overline \eta)}\big].
\end{gather}

Теперь уже можно легко заметить, что в рамках Основного Дисперсионного Тождества гипотезу можно сформулировать следующим образом:
\begin{gather}
    \label{MANOVA_VI_H}
    \mathrm H_0:
    \mathbb E \big[(\mathbb E (\overline \eta \, | \, \xi) - \mathbb E \overline \eta)
              \Tr{(\mathbb E (\overline \eta \, | \, \xi) - \mathbb E \overline \eta)}\big]
    = 0.
\end{gather}
Эта запись является многомерным обобщением \eqref{ANOVA_VI_H} и так же означает, что средние внутри групп не отличаются от общего среднего.

Выборка задается так же как в ANOVA, но теперь индивиды многомерные.
Имеется $n_i$ индивидов из $i$-той группы ($i \in 1:k$).
Обозначим $\mathbf y_{ij} \in \R^p$\footnote{\color{blue} Шрифт тут неудачный, но пусть пока так будет}
--- $j$-того индивида из $i$-той группы ($i \in 1:k$ и $j \in 1:n_i$).
Обозначим $\overline {\mathbf y}$ --- выборочное среднее по всем индивидам, а $\overline {\mathbf y_i}$
--- выборочное среднее индивидов $i$-той группы.

Запишем \eqref{MANOVA_VI_gen} на выборочном языке:
\begin{gather}
    \label{VI_MANOVA}
    \sum_{i=1}^k \sum_{j=1}^{n_i} (\mathbf y_{ij} - \overline {\mathbf y}) \Tr{(\mathbf y_{ij} - \overline {\mathbf y})} =
    \sum_{i=1}^k \sum_{j=1}^{n_i} (\mathbf y_{ij} - \overline {\mathbf y_i}) \Tr{(\mathbf y_{ij} - \overline {\mathbf y_i})} +
    \sum_{i=1}^k n_i (\overline {\mathbf y_i} - \overline {\mathbf y}) \Tr{(\overline {\mathbf y_i} - \overline {\mathbf y})}.
\end{gather}
Должно быть ясно, что это лишь многомерное обобщение \eqref{VI_ANOVA}.

Следуя обобщенным обозначениям, предложенным в вопросе 10, можно ввести следующие:

\begin{gather*}
    \mathrm {\mathbb E} \overset{\mathrm{def}}{=}
    \widehat {\mathbb E \big[(\overline \eta - \mathbb E (\overline \eta \, | \, \xi))
    \Tr{(\overline \eta - \mathbb E (\overline \eta \, | \, \xi))}\big]} =
    \sum_{i=1}^k \sum_{j=1}^{n_i} (\mathbf y_{ij} - \overline {\mathbf y_i}) \Tr{(\mathbf y_{ij} - \overline {\mathbf y_i})};\\
    \mathrm {\mathbb H} \overset{\mathrm{def}}{=}
    \widehat{\mathbb E \big[(\mathbb E (\overline \eta \, | \, \xi) - \mathbb E \overline \eta)
    \Tr{(\mathbb E (\overline \eta \, | \, \xi) - \mathbb E \overline \eta)}\big]} =
    \sum_{i=1}^k n_i (\overline {\mathbf y_i} - \overline {\mathbf y}) \Tr{(\overline {\mathbf y_i} - \overline {\mathbf y})}.
\end{gather*}

Стоит сразу отметить, что, когда в дальнейшем мы будем строить критерии для проверки значимости,
мы всегда будем их строить, как некие преобразования над парой матриц $(\mathbb H, \mathbb E)$.

\subsection{Билет 16. MANOVA для дискриминантного анализа и для многомерной множественной регрессии, общее и различие.}

Будем использовать обозначения Билета 15.

\paragraph{Дискриминантный анализ}
Дискриминатный анализ работает с $k$ группами и $p$ индивидами\footnote{Обозначения вводятся, чтобы потом сформулировать критерий.}.
Есть несколько задач: проверка того, что $k$ групп различаются (иными словами
проверка значимости), далее объяснение различия и далее классификация.

Использование MANOVA в первой задаче совершенно естественно (в принципе, можно использовать и другие критерии,
например, непараметрические, но нас это сейчас не интересует).

Гипотеза будет иметь вид \eqref{MANOVA_VI_H}.

\paragraph{Многомерная множественная регрессия}

В случае многомерной множественной регрессии все аналогично одномерной множественной регрессии.
Мы хотим проверить гипотезу, что регрессия незначима.
%\begin{gather*}
%    \mathbb Y = \mathbb {X B} + \Xi.
%\end{gather*}
%Причем $\mathbb Y \in \mathrm M_{n, p}(\R)$, $\mathbb X \in \mathrm M_{n, k}(\R)$, $\mathbb B \in M_{k, p}(\R)$.
%Строчки $\Xi$ являются независимыми, при этом $\Xi \sim \mathcal N(\mu_i, \Sigma)$.
%{\color{blue} Кто знает, пусть подправит.}
%Заметим, что в такой постановке задачи $n$ является параметром модели.
%
%Это удобно при стандартных линейнорегрессионных рассуждениях,
%но нам сейчас удобно будет отказаться от такого предположения и записать модель в следующем виде:
Уточним смысл сказанного. Во-первых, вспомним, что по определению регрессия --- УМО.
Во-вторых, какая бы модель у нас ни была, в стандартной ситуации нам нужно от этой модели только, чтобы УМО было линейным
(это будет ясно чуть далее).
Существует несколько моделей, в которых УМО --- линейно. Приведем примеры двух из них.

\begin{gather*}
    \overline \eta = \mathbb B \overline \xi + \overline \varepsilon,
\end{gather*}
где $\overline \xi \in \R^k$, $\overline \eta \in \R^p$, $\overline \eps \in \R^p$ ---
случайные вектора, $\mathbb B \in \mathrm M_{p, k}(\R)$ --- матрица коэффициентов,
причем $\overline \xi$, $\overline \varepsilon$ --- независимы и $\mathbb E \overline \varepsilon 0$.

В такой модели нетрудно посчитать, что $\mathbb E(\overline \eta \, | \, \overline \xi) = \mathbb B \overline \xi$,
то есть регрессия является линейной, нас это устраивает.

Примером другой модели такого типа является следующая: $\Tr{(\overline \eta, \overline \xi)} \sim \mathcal N$ с
какими-то параметрами, которые нас сейчас не интересуют. Важно то, что в такой модели
тоже найдется матрица $\mathbb B$: $\mathbb E(\overline \eta \, | \, \overline \xi) = \mathbb B \overline \xi$.

В обеих моделях гипотеза о незначимости формулируется так: $\mathrm H_0: \mathbb B = 0$.
Именно здесь используется то, что мы требуем линейности регрессии (найдется такая матрица!).

Учитывая то обстоятельноство, что для независимых $\xi_1, \xi_2$ справедливо, что $\mathbb E (\xi_1 \, | \, \xi_2) = \mathbb E\xi_1$,
гипотеза о незначимости регрессии для моделей можно свести к другой, сформулированной в рамках Основного Дисперсионного Тождества
\eqref{VI}:
\begin{gather*}
    \mathrm H_0:
    \mathbb E \big[(\mathbb E (\overline \eta \, | \, \xi) - \mathbb E \overline \eta)
              \Tr{(\mathbb E (\overline \eta \, | \, \xi) - \mathbb E \overline \eta)}\big]
    = 0.
\end{gather*}

То есть при независимости $\overline \xi$ и $\overline \eta$ обе гипотезы эквивалентны.
Заметим, что вторая гипотеза является гораздо более общей, в ней не нужны предположения о линейности регрессии!
Обратим внимание, что эта гипотеза полностью совпадает по своему виду с гипотезой \eqref{MANOVA_VI_H},
которая проверяется с помощью MANOVA.

\bigskip
Таким образом, проверку значимости дискриминантного анализа и многомерной множественной линейной регрессии можно проводить с помощью
MANOVA.

\paragraph{Построение критерия}
Перейдем к построению критерия. Для этого перейдем на выборочный язык.

Оценкой $\mathbb E(\mathbb E(\eta\, | \, \xi) - \mathbb E \xi)^2$ является матрица $\mathbb H$.
Сначала нам потребуется следующее утверждение.
\begin{thm}[Без доказательства]
    \label{Wilks-thm-gen}
    Пусть $\mathbb A \sim \mathcal W_p(\Sigma_\mathbb A, \nu_\mathbb A)$ и
    $\mathbb B \sim \mathcal W_p(\Sigma_\mathbb B, \nu_\mathbb B)$, причем $\Sigma_\mathbb B$ --- невырождена\footnote{
    Напомню, что это означает, что распределение сосредоточено на всем пространстве положительно определенных матриц.}
    и $\mathbb A$ и $\mathbb B$ --- независимы.
    Обозначим $\lambda_i$ --- $i$-тое по упорядоченности по невозрастанию собственное число матрицы $\mathbb B^{-1} \mathbb A$.
    Тогда
    \begin{gather*}
        \mathbb {\frac{|B|}{| A + B |}} = \frac{1}{| 1 + \mathbb B^{-1} \mathbb A|} = \prod_{i=1}^p {\frac{1}{1 + \lambda_i}}
    \sim \Lambda_1(\nu_\mathbb A, \nu_\mathbb B).
    \end{gather*}
\end{thm}

Заметим, что оба равенства являются фактами из линейной алгебры, причем первое равенство легко интерпретируется тем, что
определитель мультипликативен, а $\mathbb B$ является положительно определенной. Второе равенство легко понять, если подумать
о Жордановой Нормальной Форме матрицы $\mathbb B^{-1} \mathbb A$.

Из этого утверждения подстановкой нужных букв следует то, что позволяет сформулировать критерий.

\begin{thm}
    \label{Wilks-thm-conc}
    Пусть матрица $\mathbb H \sim \mathcal W_p(\Sigma_\mathbb H, \nu_\mathbb H)$ и
    $\mathbb E \sim \mathcal W_p(\Sigma_\mathbb E, \nu_\mathbb E)$, причем $\mathbb H$ и $\mathbb E$ независимы.
    Обозначим $\lambda_i$ --- $i$-тое по упорядоченности по невозрастанию собственное число матрицы $\mathbb E^{-1} \mathbb H$.
    Тогда
    \begin{gather*}
       \mathbb {\frac{|E|}{| H + B |}} = \frac{1}{| 1 + \mathbb E^{-1} \mathbb H|} = \prod_{i=1}^s {\frac{1}{1 + \lambda_i}}
    \sim \Lambda_1(\nu_\mathbb H, \nu_\mathbb E),
    \end{gather*}
    где $s = \min(\nu_\mathbb H, p)$.

    В случае дискриминантного анализа $\nu_\mathbb H = k - 1$, $\nu_\mathbb E = n - k$.
    В случае многомерной множественной регрессии $\nu_\mathbb H = k$, $\nu_\mathbb E = n - k - 1$.
\end{thm}

\begin{proof}
   То, что $s$ выглядит так, объясняется в вопросе 20.
   То, что число степеней свободы такое, во-первых, можно увидеть непосредственно, во-вторых, по аналогии с одномерным случаем.
\end{proof}

Заметим, что построенный критерий для проверки соответствующей гипотезы является далеко не единственным.
О построении других критериев речь идет в вопросе 19.


\subsection{Билет 17. Какой смысл у канонических дискриминантных функций (коэффициентов) и переменных?}

В вопросе 16 мы построили критерий для проверки значимости дискриминантного анализа.
На самом деле это не единственный возможный критерий.
Главными объектами, с которыми приходится работать, являются матрицы $\mathbb H$ и $\mathbb E$.
На основе разных способов <<совмещения>> этих объектов могут получаться разные критерии.
Подробнее об этом в билете 19.

В этом билете подробнее изучаются свойства этих двух матриц.

\bigskip

Напомним, что имеется $p$ признаков и $k$ групп: $\overline \eta \in \R^p$, причем $\mathcal P(\overline \eta \, | \, \xi = A_i) =
\mathcal N(\mu_i, \Sigma)$.
Попытаемся на основе имеющихся $p$ признаках построить новый, по которому группы <<наиболее бы отличались>>.
Эта неформальная задача на самом деле очень просто формализуется, как мы увидем несколько далее.

Запишем на генеральном языке, что значит <<новый признак>>. Есть $A \in \R^p$ и новый признак
$\zeta = \Tr{A} \overline \eta$: $\mathcal P(\zeta \, | \, \xi = A_i) = \mathcal N(\Tr{A} \mu_i, \Tr{A} \Sigma A)$.
На выборочном языке --- $Z = \mathbb Y A$ и выборочная ковариационная матрица (с точностью до коэффициента имеет вид):
$\Tr{A} \Tr{\mathbb Y} \mathbb Y A = \Tr{A} ( \mathbb H + \mathbb E ) A$. Из этого следует\footnote{\color{blue} Кто-то умеет формально
это доказывать? Это нужно делать как-то на генеральном языке видимо, пользуясь ортогональностью.}, что аналогом $\mathbb H$ для
нового признака является $\Tr{A} \mathbb H A$, а аналогом $\mathbb E$ является $\Tr{A} \mathbb E A$.

Посмотрим теперь на $F$-статистику, следуя обобщенному определению статистики, данному в билете 10:

\begin{gather*}
    F = F(A) = C \frac{\mathrm{SSH}_\zeta}{\mathrm{SSE_\zeta}} = C \frac{\Tr{A} \mathbb H A}{\Tr{A} \mathbb E A} \sim
    \mathrm F_{\nu_\mathbb H, \nu_\mathbb E}.
\end{gather*}
где $C = \nu_\mathbb E / \nu_\mathbb H$ --- коэффициент, не зависящий от $A$.

Исходно ставилась задача --- найти такой признак, по которому группы <<наиболее бы отличались>>.
В терминах статистики $F$ --- это означает\footnote{Приглашаю убедиться.}, что решается оптимизационная задача
\begin{gather*}
    \frac{\Tr{A} \mathbb H A}{\Tr{A} \mathbb E A} \rightarrow \max_A.
\end{gather*}

Как легко понять --- эта задача является обобщенной задачей на собственные числа и собственные вектора.
Собственные числа матрицы $\mathbb E^{-1} \mathbb H$ в порядке невозрастания: $\lambda_1 \geqslant \lambda_2 \geqslant \ldots
\geqslant \lambda_s$, где $s = \min\{p, \nu_H\}$, и собственные вектора той же матрицы $A_i$ для $i \in 1:s$.
Причем $\Tr{A_i} \mathbb E A_j = 0$ для $j < i$.
Вектора $A_i$ --- канонические коэффициенты (дискриминантные функции). Новые признаки $Z_i = \mathbb Y A_i$ --- канонические переменные.
В дальнейшем, оказывается, что новые признаки являются ортогональными.

Исходя из описания, смысл $\lambda_i$ --- степень разброса по $i$-тому направлению, задаваемым $i$-той дискриминантной функцией.
$A_i$ --- коэффициенты с которыми нужно взять исходные признаки, чтобы получить новый признак, имеющий наибольший разброс и ортогональный
предыдущим. А сами канонические переменные тем самым --- ортогональные новые признаки, имеющие наибольший разброс, измеряемый $\lambda_i$.

\subsection{Билет 18. Как вычисляются канонические дискриминантные функции (коэффициенты)?}

Если посмотреть в вопрос 17, то ясно, что дискриминантные функции вычисляются как собственные вектора матрицы $\mathbb E^{-1} \mathbb H$.

\subsection{Билет 19. Значимость LDA. Разные критерии, чем отличаются.}

Будем использовать обозначения Билета 15. 
Как уже говорилось в вопросе 16, дискриминатный анализ работает с $k$ группами и $p$ индивидами
Есть несколько задач: проверка того, что $k$ групп различаются (иными словами
проверка значимости), далее объяснение различия и далее классификация.
Использование MANOVA для проверки значимости дискриминантного анализа совершенно естественно.
Один из критериев \ref{Wilks-thm-conc} уже был построен --- этот критерий использует
общее утверждение \ref{Wilks-thm-gen}.
На самом деле это далеко не единственный критерий, который может быть построен, в частности, для проверки значимости дискриминантного анализа.
Как уже отмечалось, главными объектами, с которыми приходится работать, являются матрицы $\mathbb H$ и $\mathbb E$.
На основе разных способов <<совмещения>> этих объектов могут получаться разные критерии.
Рассмотрим несколько способов. Напомним, что $s = \min\{\nu_\mathbb H, p\}$.

Проверяется гипотеза о равенстве средних\footnote{На самом деле о равенстве распределений в рамках нормальной модели при условии гомоскедастичности.}

\begin{gather*}
    \mathrm H_0: \mathbb E \overline \eta_1 = \mathbb E \overline \eta_2 = \ldots = \mathbb E \overline \eta_k.
\end{gather*}
\begin{enumerate}
    \item Lambda Wilks's
        \begin{gather*}
            \Lambda = \frac{|\mathbb{E}|}{|\mathbb E + \mathbb H|} = \frac{1}{|1 + \mathbb E^{-1} \mathbb H|} = \prod_{i=1}^s \frac{1}{1 + \lambda_i}
            \sim \Lambda_p(\nu_\mathbb H, \nu_\mathbb E).
        \end{gather*}
    \item Roy's largest root
        \begin{gather*}
            Q = \frac{\lambda_1}{1 + \lambda_1} = r_1^2.
        \end{gather*}
        Этот тест использует только первое собственное число матрицы $\mathbb E^{-1} \mathbb H$.
        Напомним, что $r_i^2$ --- называют $i$-тым каноническим корнем. Как мы увидим в вопросе 42, на самом деле $r_i^2$ --- канонические корреляции\footnote{
        При повторном чтении спросите себя: <<между чем и чем это корреляции?>>.}.
        Потому они на самом деле имеют интерпретируемый смысл.
    \item Pillai's:
        \begin{gather*}
            V^{(s)} = \mathrm {tr} \left(\frac{\mathbb H}{\mathbb E + \mathbb H}\right) = \sum_{i=1}^s r_i^2.
        \end{gather*}
    \item Hotelling:
        \begin{gather*}
            U^{(s)} = \mathrm {tr} \left(\mathbb E^{-1} \mathbb H \right) = \sum_{i=1}^s \lambda_i^2.
        \end{gather*}
\end{enumerate}

Как всегда, когда мы видим много критериев для проверки одной гипотезы, нужно научиться их сравнивать.
Сравнивают критерии по мощности. Но, чтобы говорить о мощности нужно фиксировать альтернативу.
Таким образом, вопрос можно поставить следующим образом: <<Для каких альтернатив каждый\footnote{Или хотя те из них, про которые это легко понять.}
из предложенных критериев мощнее остальных?>>.

Посмотрим на это на примере первых двух критериев.
Первый критерий включает в статистику все (!) направления, по которым разброс максимален (иными словами, все $\lambda_i$)\footnote{Смысл раскрыт в вопросе 17.}.
Второй же критерий включает в себя лишь первое направление, с максимальной степенью разброса, измеряемой $\lambda_1$.
Из этого становится ясно, что если у нас на самом деле лишь одно направление определяет разброс (например, $k$ шариков лежат на одной прямой),
то остальные $\lambda_i$ при $i > 2$ уже не отображают различие между группами и потому в такой ситуации следует ожидать, что второй критерий окажется мощнее первого.
В обратном случае, когда все направления описывают различие между группами, следует ожидать, что первый критерий окажется мощнее второго.

Про третий и четвертый критерий на лекции утверждалось, что они <<где-то по середине>> между первым и вторым.\footnote{{\color{blue} Более полные и точные объяснения приветствуются.}}

В заключение заметим, что у всех критериев кроме первого критическая область находится <<слева>> и около 0 (все статистики неотрицательны),
у первого критерия все наоборот: носитель статистики от 0 до 1 и критическая область расположена около 1.

\subsection{Максимальное число дискриминантных функций, почему такое?}

Пользуемся обозначениями из билетов 15 и 16. Здесь мы рассматриваем именно случай применения MANOVA к дискриминантному анализу.

Ясно, что $s \leqslant \rk(\mathbb E^{-1} \mathbb H)$ (за $s$-тым собственным числом, если и <<идут>>, то точно нулевые собственные числа, нам такие не интересны).
Далее, размеры обеих матриц $p \times p$, но $\mathbb E$, очевидно, полного ранга, поэтому\footnote{\color{blue} Это какое-то стандартное свойство матриц: домножение
на обратимую не меняет ранга матрицы.}
$s = \rk{\mathbb H}$. Далее, на матрицу $\mathbb H$ можно смотреть (с точностью до коэффициентов), как на оценку ковариационной матрицы из центров групп (выборка размера $k$).
То есть можно сказать, что есть исходная матрица данных центров групп $\tilde Y \in \mathrm M_{p, k}(\R)$ и $\rk(\mathbb H) = \rk(\mathbb {\tilde Y} - \mathbb{\overline Y}) \leqslant \min\{p, k\}$,
где $\overline {\mathbb Y}$ --- усредненная матрица.
Но на самом деле $\rk(\mathbb {\tilde Y} - \mathbb{\overline Y}) \leqslant \min\{p, k - 1\}$, из-за усреднения. Докажем это. По определению
\begin{gather*}
    \mathbb H = \sum_{i=1}^k n_i (\mathbf {\overline y}_{i} - \mathbf {\overline y}) \Tr{(\mathbf {\overline y}_{i} - \mathbf {\overline y})}.
\end{gather*}

При этом известно, что $\sum_{i=1}^k n_i \mathbf {\overline y}_i = n \mathbf {\overline y}$, где $n = \sum_{i=1}^k n_i$.
То есть $\sum_{i = 1}^k n_i (\mathbf {\overline y}_i  - \mathbf {\overline y}) = 0$, тем самым вектора $\{\mathbf {\overline y}_i  - \mathbf {\overline y}\}_{i=1}^k$ линейнозависимы,
Но ведь матрица $\mathbb H$ --- сумма матриц ранга 1, из которых одна представляется линейной комбинацией других, а значит ее ранг не превосходит $k - 1$.

Таким образом, можно положить $s = \min\{p, k - 1\}$.

Обращаю внимание, что $s$ --- это не максимальное количество собственных чисел матрицы $\mathbb E^{-1} \mathbb H$, и даже не максимальное число ненулевых с.ч.
Реально ненулевых собственных чисел может оказаться и меньше, чем $s$, но главное, что никогда не окажется, что ненулевых собственных чисел больше, чем $s$.
