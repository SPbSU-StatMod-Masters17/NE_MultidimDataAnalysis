% \documentclass[12pt,a4paper]{article}

% \include{commons/packages/abstract}        % Подключаемые пакеты
% \include{commons/styles/abstract}          % Пользовательские стили
% \include{commons/commands/commands}  
% \begin{document}


\subsection{Билет 21. С чем совпадают дискриминантные функции и переменные, если ошибки сферические?}

	Пусть $\mathbb Y \in M_{n, p}\left(\mathbb R\right)$ --- матрица данных (строки $\mathbf y_i, i \in 1\mathbin : n$ -- наблюдения, столбцы $Y_j, j\in 1\mathbin p$ -- признаки), 
	наблюдения принадлежат к одной из $k$ групп, 
	в каждой группе $n_i$ наблюдений, $n = \sum_{i=1}^k n_i$. %%$\Tr{\left(abc\right)}$
	Матрица межгрупповых отклонений $\mathbb H = \sum_{i=1}^k n_i\left(\bar{\mathbf y}_i - \bar{\mathbf y}\right)\Tr{\left(\bar{\mathbf y}_i - \bar{\mathbf y}\right)}$, 
	матрица внутригрупповых отклонений $\mathbb E = \sum_{i=1}^k\sum_{j=1}^{n_i} \left(\mathbf y_{ij} - \bar{\mathbf y}_i\right)\Tr{\mathbf y_{ij} - \bar{\mathbf y}_i}$.

	В случае, если $\mathbb E = \sigma^2\mathbb I$ (все группы сферические и одинакового размера), дискриминантные функции данных, т.е. $A_1, \ldots, A_s$ -- собственные вектора $\mathbb E^{-1}\mathbb H$, становятся собственными векторами матрицы $\sigma^2\mathbb H$. То есть дискриминантные функции являются собственными векторами матрицы межгрупповых ковариаций. Так они совпадают с главными направлениями в терминах анализа главных компонент, если каждую группу рассматривать как одного индивида (в АГК главные направления --- это собственные вектора ковариационной матрицы), а дискриминантные переменные $\mathbb Y A_i$ при такой постановке являются главными компонентами.

\subsection{Билет 22. Как определить значимое число дискриминантных функций (размерность пространства, где группы различаются).}

	Модель дискриминантного анализа (на генеральном языке): $\xi$ -- дискретная с.в. с носителем $\left\lbrace A_i\right\rbrace_{i=1}^k$, $\eta = \eta_i$, если $\xi = A_i$. Определены матрицы
	$$\mathbb H = \mathrm E\left(\left(\mathrm E\left(\eta\middle\vert\xi\right) - \mathrm E\eta\right)\Tr{\left(\mathrm E\left(\eta\middle\vert\xi\right) - \mathrm E\eta\right)}\right) \in M_p\left(\R\right),$$
	$$\mathbb E = \mathrm E\left(\left(\eta - \mathrm E\left(\eta\middle\vert\xi\right)\right)\Tr{\left(\eta - \mathrm E\left(\eta\middle\vert\xi\right)\right)}\right) \in M_p\left(\R\right).$$

	Дискриминантные функции $A_1, \ldots, A_s$ --- это с.в. матрицы $\mathbb E^{-1} \mathbb H$, $s = \min\left\lbrace p, \nu_H \right\rbrace$.

	Для того, чтобы узнать, какие из функций являются значимыми, мы последовательно проверяем гипотезу $H_0$: <<дискриминантные функции $A_m, \ldots, A_s$ не описывают различия в данных>> для $m\in 1\mathbin : s$.  Мы можем позволить себе проверять её последовательно, так как знаем, что дискриминантные функции, будучи отсортированы по убыванию соответствующих собственных чисел, оказываются в порядке убывания качества объяснения различий (см. билет 2.13).

	Если $H_0$ верна, то $\mathrm{rk}\mathbb H = m - 1$, и (почему?) $$\Lambda'_m = \prod_{i=m}^s \frac{1}{1+\lambda_i} \sim \Lambda_p\left(\nu_H + (m-1), \nu_E - (m-1)\right).$$

	С помощью статистики $\Lambda'_m$ (lambda prime) мы для каждого $m$ можем проверить, незначимы ли эта и все последующие дискриминантные функции.

\subsection{Билет 23. Почему канонические дискриминантные переменные получаются ортогональными.}
	Пусть $\mathbb Y \in M_{n, p}\left(\mathbb R\right)$ --- матрица данных (строки $\mathbf y_i, i \in 1\mathbin : n$ -- наблюдения, столбцы $Y_j, j\in 1\mathbin p$ -- признаки), 
	наблюдения принадлежат к одной из $k$ групп, 
	в каждой группе $n_i$ наблюдений, $n = \sum_{i=1}^k n_i$. %%$\Tr{\left(abc\right)}$
	Матрица межгрупповых отклонений $\mathbb H = \sum_{i=1}^k n_i\left(\bar{\mathbf y}_i - \bar{\mathbf y}\right)\Tr{\left(\bar{\mathbf y}_i - \bar{\mathbf y}\right)}$, 
	матрица внутригрупповых отклонений $\mathbb E = \sum_{i=1}^k\sum_{j=1}^{n_i} \left(\mathbf y_{ij} - \bar{\mathbf y}_i\right)\Tr{\mathbf y_{ij} - \bar{\mathbf y}_i}$.

	$A_1, \ldots, A_s$ --- собственные вектора $\mathbb E^{-1}\mathbb H$. Дискриминантные переменные --- вектора данных в новых коородинатах $Z_i = \mathbb Y A_i$.

	\begin{thm}
		$$\forall i \neq j \in 1\mathbin : s\; A_i \perp A_j$$
	\end{thm}
	\begin{proof}
		$\Inner{Z_i}{Z_j} = \Inner{\mathbb Y A_i}{\mathbb Y A_j} = \Tr{A_i}\Tr{\mathbb Y}\mathbb Y A_j = \Tr{A_i}\left(\mathbb H + \mathbb E\right) A_j = \Tr{A_i}\mathbb E\left(\mathbb I + \mathbb E^{-1}\mathbb H\right) A_j.$ Так~как $\forall \mathbb C \geq 0$ с с.~ч. $\lambda_1, \ldots, \lambda_s$ с.~ч. матрицы $\mathbb I + \mathbb C$ --- это $1 + \lambda_1, \ldots, 1+ \lambda_s$, а собственные вектора те же самые, $\left(\mathbb I + \mathbb E^{-1}\mathbb H\right) A_j = \left(1 + \lambda_j\right) A_j$ по определению собственного вектора. Так как собственные вектора разных с.ч. ортогональны, $\Inner{Z_i}{Z_j} = \left(1+\lambda_j\right)\Tr{A_i} A_j = 0$.
	\end{proof}

\subsection{Билет 24. Интерпретация разделения: стандартизованные дискриминантные функции и факторная структура}

	Пусть $\mathbb Y \in M_{n, p}\left(\mathbb R\right)$ --- матрица данных (строки $\mathbf y_i, i \in 1\mathbin : n$ -- наблюдения, столбцы $Y_j, j\in 1\mathbin p$ -- признаки), 
	наблюдения принадлежат к одной из $k$ групп, 
	в каждой группе $n_i$ наблюдений, $n = \sum_{i=1}^k n_i$. %%$\Tr{\left(abc\right)}$
	Матрица межгрупповых отклонений $\mathbb H = \sum_{i=1}^k n_i\left(\bar{\mathbf y}_i - \bar{\mathbf y}\right)\Tr{\left(\bar{\mathbf y}_i - \bar{\mathbf y}\right)}$, 
	матрица внутригрупповых отклонений $\mathbb E = \sum_{i=1}^k\sum_{j=1}^{n_i} \left(\mathbf y_{ij} - \bar{\mathbf y}_i\right)\Tr{\mathbf y_{ij} - \bar{\mathbf y}_i}$.

	$A_1, \ldots, A_s$ --- собственные вектора $\mathbb E^{-1}\mathbb H$. Дискриминантные переменные --- вектора данных в новых коородинатах $Z_i = \mathbb Y A_i$.

	Задача: проанализировать разложение по дискриминантным переменным.

	\subsubsection{Стандартизованные дискриминантные функции}
		Первый способ проинтерпретировать разложение по дискриминантным переменным --- посмотреть на коэффициенты, с которыми исходные переменные входят в дискриминантные. Если исходные переменные измерены в различных шкалах, то коэффициенты в векторе $A_i$ одновременно ещё и приводят показатели к нужной шкале. Чтобы избежать этого эффекта, можно посмотреть на стандартизованные дискриминантные функции.

		Пусть $\mathbb S$ -- матрица взвешенных ковариаций, $s_1^2, \ldots, s_p^2$ -- элементы на её диагонали (взвешенные дисперсии признаков), $A_i = \begin{pmatrix} a_{i1}\\ \vdots\\a_{ip}\end{pmatrix}$. Тогда

		$$Z_i = \mathbb Y A_i = \sum_{j=1}^p Y_j a_{ij} = \sum_{j=1}^p \frac{Y_j}{s}\cdot s a_{ij},$$

		и $\tilde A_i = \left(\mathrm{diag}\mathbb S\right)^{\frac{1}{2}}A_i$ --- $i$-я стандартизованная дискриминантная функция (стд.~д.~ф.). Коэффициенты стд.~д.~ф. показывают вклады исходных признаков в дискриминантные переменные.

		Важно учитывать, что
		\begin{enumerate}
			\item стд.~д.~ф. могут существенно меняться при удалении/добавлении новых признаков,
			\item на малых выборках (при малом отношении $N / p$) стд.~д.~ф. нестабильны от выборки к выборке.
		\end{enumerate}

	\subsubsection{Факторная структура}
		Факторная структура --- матрица корреляций между исходными и каноническими переменными. У Ренчера указано, что им же показано, что корреляции показывают вклад исходных признаков в дискриминантные переменные независимо от вклада других признаков, устойчивы относительно удаления/добавления новых признаков и этим плохи, так как цель дискриминантного анализа -- выявить совместные влияния признаков.

\subsection{Билет 25. Свойства исходных признаков, по которым можно понять, какие признаки лишние.}
	<<Плохие>> признаки, это признаки, которые:
	\begin{enumerate}
		\item Являются линейной комбинацией других признаков, т.е. имеют большой коэффициент множественной корреляции $R^2 = R^2\left(\eta^{(i)}; \left\lbrace\eta^{(i)}\middle\vert j\in 1\mathbin : p \ \left\lbrace i \right\rbrace\right\rbrace\right)$. Соответствующая характеристика -- tolerance $= 1 - R^2$.
		\item При удалении из модели не влияют на качество разделения. Соответствующая гипотеза $H_0$: <<добавление признака $i$ не влияет на качество разделения>>. Статистика: 
		\begin{equation}\label{eq:partial lambda}
			\left(\text{Partial } \Lambda\right)_i = \frac{\Lambda\left(Y_1,\ldots, Y_p\right)}{\Lambda\left(Y_1,\ldots,Y_{i-1}, Y_{i+1}, \ldots, Y_p\right)} = \frac{1}{1 + \lambda_i}\sim \Lambda_1\left(\nu_H, \nu_E - p +1\right)
		\end{equation}
	\end{enumerate}

\subsection{Билет 26. Пошаговый дискриминантный анализ}
	Пошаговый дискриминантный анализ подбирает тот набор признаков, который лучше всего будет разделять переменные (аналогично пошаговой множественной регрессии). На каждом шаге добавляется одна переменная, которая максимально увеличивает качество разделения групп (качество меряется статистикой partial lambda из \eqref{eq:partial lambda}, точнее, из-за наличия точного преобразования из $\Lambda_1$ в $F$, эквивалентной статистикой с распределением Фишера). После этого набранная модель пересматривается на предмет наличия избыточных переменных. Процедура останавливается, когда максимальное значение $F$ для вновь добавляемых переменных не превосходит наперёд заданного порога.

	Первой переменной в модели становится та, для которой $F$-статистика из ANOVA оказалась наибольшей.

\subsection{Билет 27. Что уменьшается с помощью lambda prime и что с помощью partial lambda?}
	\subsubsection{Lambda prime}
		$H_0$: <<дискриминантные функции $A_m, \ldots, A_s$ не описывают различия в данных>>
		
		$$\Lambda'_m = \prod_{i=m}^s \frac{1}{1+\lambda_i} \sim \Lambda_p\left(\nu_H + (m-1), \nu_E - (m-1)\right)$$

		Если $H_0$ не верна, то $m$-я \emph{дискриминантная функция} значима при описании различий. Т.~е. $\Lambda'_m$ уменьшает число дискриминантных функций.

	\subsubsection{Partial lambda}
		$H_0$: <<добавление признака $i$ не влияет на качество разделения>>

		$$\left(\text{Partial } \Lambda\right)_i = \frac{\Lambda\left(Y_1,\ldots, Y_p\right)}{\Lambda\left(Y_1,\ldots,Y_{i-1}, Y_{i+1}, \ldots, Y_p\right)} = \frac{1}{1 + \lambda_i}\sim \Lambda_1\left(\nu_H, \nu_E - p +1\right)$$

		Если $H_0$ не верна, то $i$-й \emph{исходный признак }значим при описании различий исходных данных. Т.~е. $\left(\text{Partial } \Lambda\right)_i$ уменьшает число исходных признаков.


% \end{document}