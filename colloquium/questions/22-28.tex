\\
{\color{blue} Начиная отсюда и до конца 28 билета набирала Лиза.}
\begin{notation}
$\mathbb{Y} \in \mathbb{R}^{K\times L}$. 
SVD $\mathbb{Y}$ называется
\begin{gather*}
\mathbb{Y} = \sum_{i = 1}^{d} \sqrt{\lambda_i}U_iV_i^{\rm T},
\end{gather*}
где 
\begin{enumerate}
\item
$\lambda_i$ --- сингулярные числа $\mathbb{Y}$ (собственные числа $\mathbb{Y}\mathbb{Y}^{\rm T}$),
\item
$U_i$ --- левые сингулярные вектора (собственные вектора $\mathbb{Y}\mathbb{Y}^{\rm T}$),
\item
$V_i$ --- правые сингулярные вектора ($V_i = \mathbb{Y}^{\rm T}U_i/\sqrt{\lambda_i}$),
\end{enumerate}
\end{notation}

\begin{notation}
$\mathbb{X} \in \mathbb{R}^{K\times L}$ --- матрица признаков.
$\mathbb{Y} = \mathbb{X}^{\rm T}$.

Рассмотрим измеримые пространства $({\rm D}_1, \mathfrak{A}_1, \mu_1)$ и $({\rm D}_2, \mathfrak{A}_2, \mu_2)$, где
\begin{enumerate}
\item
${\rm D}_1 = \{1,\ldots, L\}$, ${\rm D}_2 = \{1,\ldots, K\}$, 
\item
$\mathfrak{A}_{1, 2}$ --- множества всех подмножеств ${\rm D}_{1,2}$, 
\item
$\mu_1$ --- считающая мера, $\mu_2$ --- вероятностная, $\mu_2(\{i\}) = 1/K$.
\end{enumerate}
Если 
\begin{gather*}
\mathbb{Y} = \sum_{i = 1}^{d} \sqrt{\tilde{\lambda_i}}\tilde{U_i}\tilde{V_i}^{\rm T}
\end{gather*}
--- SVD $\mathbb{Y}$, то 
PCA $\mathbb{Y}$ называется
\begin{gather*}
\mathbb{Y} = \sum_{i = 1}^{d} \sqrt{\lambda_i}U_iV_i^{\rm T},
\end{gather*}
где
$\lambda_i = \tilde{\lambda_i}/K$,
$U_i = \tilde{U_i}$,
$V_i = \sqrt{K}\tilde{V_i}$.
\end{notation}

\begin{note}
Об обозначениях. Считается, что
$\mathbb{A}$ --- это матрица, $\{A_i\}$ --- столбцы матрицы $\mathbb{A}$, $\{a_{ij}\}$ --- ее элементы.

Также $p$ --- это количество признаков, $n$ --- количество индивидов.
В ссылках на SVD $L$ соответствует $p$, $K$ соответствует $n$.  
\end{note}

\subsection{ Билет 22. На основе каких элементов сингулярного разложения интерпретируются главные компоненты как линейные комбинации исходных признаков? Привести формулу и пример}

Пусть $\mathbb{X} \in \mathbb{R}^{K\times L}$ --- матрица исходных признаков, $\mathbb{Y} = \mathbb{X}^{\rm T}$.

Главные компоненты --- это $Z_i = \sqrt{\lambda_i} V_i$, $i = 1,\ldots, d$, где $\sqrt{\lambda_i},\, V_i$ из PCA $\mathbb{Y}$.

Можно показать, что $Z_i = \mathbb{X}U_i$, где $U_i$ --- собственные вектора ковариационной матрицы
$\mathbb{X}^{\rm T}\mathbb{X}/K$.
Таким образом, главные компоненты --- это линейные комбинации исходных признаков, где в качестве коэффициентов выступают собственные вектора ковариационной матрицы исходных признаков.

\begin{ex}
Двумерный случай со стандартизованной матрицей, $U_i,\, Z_i$ легко считаются.
\begin{gather*}
\frac{1}{K}\ \mathbb{X}^{\rm T}\mathbb{X} = 
\begin{pmatrix}
  1 & \rho \\
  \rho & 1
\end{pmatrix}.
\end{gather*}
Пусть $\rho > 0$, тогда $U_1 = (1, 1)^{\rm T}/\sqrt{2}$ --- первое главное направление.
$U_1 \perp U_2$, значит, $U_2 = (1, -1)^{\rm T}/\sqrt{2}$.

Получаем, что $Z_1 = (x_{11} + x_{12}, x_{21} + x_{22})^{\rm T}$, то есть сумма исходных признаков, а  
$Z_1 = (x_{11} - x_{12}, x_{21} - x_{22})^{\rm T}$ -- их разность.

Вклад первой компоненты $(1 + \rho)/2$, вклад второй --- $(1 - \rho)/2$.

\end{ex}  

\subsection{ Билет 23. АГК с точки зрения построения базиса в пространстве индивидов и в пространстве признаков. Координаты в новых базисах}

$\mathbb{X} = [X_1:\ldots:X_L]\in \mathbb{R}^{K\times L}$ --- матрица исходных признаков, $\mathbb{Y} = [Y_1:\ldots:Y_K] = \mathbb{X}^{\rm T}$.
$L = p$, $K = n$, также считаем, что $d = p$, то есть матрица $\mathbb{Y}$ --- полного ранга.\footnote{{\bf UPD}. Добавила разъяснение.}
Вектора $U_i,\, V_i$, $i = 1,\ldots, d$ из PCA $\mathbb{Y}$.

С точки зрения построения базиса в пространстве индивидов:

$\{U_j\}_{j = 1}^{d}$ --- это ортонормированный базис в пространстве индивидов,
$z_{ij} = Y_i^{\rm T}U_j$ --- это координаты $i$-го индивида в этом базисе.\footnote{{\bf UPD}. Здесь исправила определение $z_{ij}$.}

С точки зрения построения базиса в пространстве признаков:

$\{V_j\}_{j = 1}^{d}$ --- это ортонормированный базис в пространстве признаков,
\begin{gather*}
f_{ij} = \langle\,X_i, V_j\,\rangle = 
\begin{cases}
{\rm cov}(X_i, V_j),\ \text{если считать по ковариационной матрице},\\ 
\rho(X_i, V_j),\ \text{если считать по корреляционной матрице},
\end{cases}
\end{gather*}
 --- это координаты $i$-го признака в этом базисе.

\subsection{ Билет 24. Как выявить индивидов, которые плохо описываются плоскостью первых двух главных компонент?}

Рассмотрим $Y_i$ --- $i$-индивид и первые два главных направления $U_{1,2}$.

$\cos^2(\angle(Y_i, {\rm span}(U_1, U_2))) = \left(\frac{Y_i^{\rm T}U_1}{\|Y_i\|\|U_1\|}\right)^2 + \left(\frac{Y_i^{\rm T}U_2}{\|Y_i\|\|U_2\|}\right)^2 = (z_{i1}^2 + z_{i2}^2)/\|Y_i\|^2$.

Те индивиды, для которых $\cos^2$ мал, плохо описываются плоскостью первых двух главных компонент.

\subsection{Билет 25. Как вычислить значения главных компонент для индивида, которого не было в исходной выборке. А как вычислить значения факторных значений?}

Пусть\footnote{Здесь $L = p$ и обычно $d = p$}  $A \in \mathbb{R}^p$ --- новый индивид. 
Тогда $B = A^{\rm T}\mathbb{U}$ --- значения\footnote{{\bf UPD}. Транспонирование $\mathbb{U}$ было лишним. Исправила.} главных компонент для этого индивида.

Напомню, что факторные значения $\{V_i\}$ --- это нормированные главные компоненты, поэтому
$\tilde{B} = A^{\rm T}\tilde{\mathbb{U}}$, где $\tilde{\mathbb{U}} = \mathbb{U}/\sqrt{\lambda_i}$.

\subsection{Билет 26. В каком случае координаты в ортонормированном базисе можно назвать корреляциями?}

Нужно, чтобы признак, который расскладывают, был стандартизован, а вектора в базисе --- центрированы (они уже отнормированы).


\subsection{Билет 27. Чему равны суммы по строкам и по столбцам в матрице, составленной из собственных векторов в АГК?}

Матрица, составленная из собственных векторов, это $\mathbb{U} = [U_1:\ldots:U_p] =\{u_{ij}\} \in \mathbb{R}^{p\times p}$.

Сумма квадратов по строкам --- $\sum_{i = 1}^p u_{ij}^2 = \|U_j\|^2 = 1$.
Сумма квадратов по столбцам --- $\sum_{j = 1}^p u_{ij}^2 = 1$, так как $\mathbb{U}^{\rm T} = \mathbb{U}^{-1}$.

\subsection{Билет 28. Чему равны суммы по строкам и по столбцам в матрице факторных нагрузок в АГК?}

Матрица факторных нагрузок --- это $\mathbb{F} = [F_1:\ldots:F_d] = \{f_{ij}\} \in \mathbb{R}^{p\times d}$. Вспомним, что $V_j \in \mathbb{R}^n$, как и столбцы матрицы $\mathbb{X} = [X_1,…,X_p].$. 
Далее из $SVD$-разложения мы знаем, что $U_i = \frac{YV_i}{\sqrt{\lambda_i}}$. Поэтому $\sqrt{\lambda_i}U_i = YV_i$ и $(F_i)_j = \Inner{X_j}{U_i}$. $F_i$ — i-ый столбец, поэтому 
$f_{ij} = <X_i, V_j$.  

Сумма квадратов по строкам --- $\sum_{i = 1}^p f_{ij}^2 = \|F_j\|^2 = \lambda_j$.
Сумма квадратов по столбцам
\begin{gather*}
\sum_{j = 1}^d f_{ij}^2 = \sum_{j = 1}^d \Inner{X_i}{V_j}^2 = 
\begin{cases}
 \|X_i\|^2,\ \text{если считать по ковариационной матрице},\\ 
1,\ \text{если считать по корреляционной матрице}.
\end{cases}
\end{gather*}
